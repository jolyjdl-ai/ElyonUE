# Ã‰lyonEU â€” Guide Complet de DÃ©marrage & Utilisation

**Version** : 0.3.0
**Date** : 5 novembre 2025
**Status** : âœ… OPÃ‰RATIONNEL

---

## ğŸ“‹ Vue d'ensemble

Ã‰lyonEU est une **plateforme IA locale-first** avec gouvernance 6S/6R, capable de :
- ğŸ§  **Comprendre rapidement** l'utilisateur (intentions, urgence, entitÃ©s)
- ğŸ’¾ **MÃ©moriser** le contexte conversationnel (buffer 5 tours)
- ğŸ” **Rechercher** dans la connaissance (RAG vectoriel TF-IDF)
- ğŸŒ **GÃ©nÃ©rer** localement d'abord, fallback OpenAI si demandÃ©
- ğŸ“± **Deux UIs** : Desktop (PySide6) prioritaire + Web (HTML/JS)

---

## ğŸš€ DÃ©marrage Rapide

### Option 1 : Desktop + Web (RecommandÃ©)
```powershell
cd C:\ElyonEU
.\scripts\Start-ElyonEU-All.ps1
```
Cela dÃ©marre :
1. API (Uvicorn port 8000)
2. **UI Desktop (PySide6)** â†’ Chat + Moniteur temps rÃ©el
3. UI Web (navigateur) â†’ Chat seul

### Option 2 : Desktop seul
```powershell
.\scripts\Start-ElyonEU-Desktop.ps1
```

### Option 3 : API + Web seul
```powershell
.\scripts\Start-ElyonEU.ps1
```

### Option 4 : API seul (debug)
```powershell
cd .\api
.\.venv\Scripts\python.exe C:\ElyonEU\run_api.py
```

---

## ğŸ§ª Test de la Phrase Officielle

**Phrase** :
```
Peux-tu me rÃ©sumer le plan gouvernance 6S/6R avec urgence, et proposer une prochaine Ã©tape opÃ©rationnelle alignÃ©e Ã  la gouvernance ?
```

### Via Python (recommandÃ©)
```powershell
C:\ElyonEU\api\.venv\Scripts\python.exe C:\ElyonEU\test_chat.py
```

### Via PowerShell
```powershell
$body = @{
    messages = @(
        @{
            role = "user"
            content = "Peux-tu me rÃ©sumer le plan gouvernance 6S/6R avec urgence, et proposer une prochaine Ã©tape opÃ©rationnelle alignÃ©e Ã  la gouvernance ?"
        }
    )
} | ConvertTo-Json -Depth 4

Invoke-RestMethod -Uri "http://127.0.0.1:8000/chat" `
    -Method Post `
    -Body $body `
    -ContentType "application/json" | ConvertTo-Json
```

### RÃ©ponse attendue
```json
{
  "reply": "Le plan de gouvernance 6S/6R d'ElyonEU se rÃ©sume comme suit...",
  "provider": "openai",
  "trace": {
    "policy": "local_first",
    "local_provider": "gen_local",
    "memory_used": false,
    "intent": {
      "intent": "summary_request",
      "urgent": true,
      "keywords": ["plan", "gouvernance", "6S/6R"]
    }
  }
}
```

---

## ğŸ”§ Configuration

### Variables d'environnement

**Obligatoire pour OpenAI** :
```powershell
$env:ALLOW_CLOUD = "1"
$env:OPENAI_API_KEY = "sk-..."  # Remplacer par vraie clÃ©
```

**Optionnel** :
```powershell
$env:ELYON_CHAT_PROVIDER = "openai"           # lmstudio|openai
$env:ELYON_CHAT_MODEL = "gpt-4o-mini"         # DÃ©tectÃ© auto si non fourni
$env:ELYON_CHAT_POLICY = "local_first"        # local_first|fallback|external_first
$env:ELYON_CHAT_EXTERNAL_ON_FALLBACK = "1"    # Fallback cloud si local Ã©choue
```

### Fichier de config
`config/chat_backend.json` :
```json
{
  "provider": "openai",
  "base_url": "https://api.openai.com/v1",
  "model": "gpt-4o-mini",
  "api_key": ""
}
```
Note : `api_key` doit Ãªtre dans `$env:OPENAI_API_KEY` (sÃ©curitÃ©).

---

## ğŸ“¡ Endpoints API

### Health Check
```bash
GET http://127.0.0.1:8000/health
```

### Chat (endpoint principal)
```bash
POST http://127.0.0.1:8000/chat
Content-Type: application/json

{
  "messages": [
    { "role": "user", "content": "Votre demande" }
  ],
  "temperature": 0.3,
  "max_tokens": 512,
  "rag_top_k": 3
}
```

### Ã‰vÃ©nements (monitoring)
```bash
GET http://127.0.0.1:8000/events
```
Retourne les 100 derniers Ã©vÃ©nements (CHAT, RAG, PING, etc.).

### ContrÃ´le Heartbeat
```bash
GET http://127.0.0.1:8000/control
POST http://127.0.0.1:8000/control
  { "run_pings": false, "interval_sec": 5 }
```

---

## ğŸ§  Architecture Modules

| Module | Fichier | RÃ´le |
|--------|---------|------|
| **MÃ©moire** | `api/core/memory.py` | Buffer 5 tours conversationnels |
| **Intentions** | `api/core/intent.py` | Analyse urgence, keywords, entitÃ©s |
| **RAG Vectoriel** | `api/core/vector_index.py` | Recherche TF-IDF (3 docs indexÃ©s) |
| **Configuration** | `config/chat_backend.json` | Provider, modÃ¨le, base_url |
| **API Central** | `api/elyon_api.py` | Orchestration `/chat`, Ã©vÃ©nements, fallback |
| **GÃ©nÃ©rateur local** | `app/services/generative_core.py` | GÃ©nÃ©ration templates + cloud |

---

## ğŸ“Š Flux de Traitement (`/chat`)

```
1. RequÃªte entrante
   â†“
2. Charger mÃ©moire (rÃ©sumÃ© 5 tours)
   â†“
3. Analyser intentions utilisateur
   â†“
4. Recherche RAG (vectorielle TF-IDF)
   â†“
5. Enrichir messages avec contexte
   â†“
6. GÃ©nÃ©ration LOCALE (d'abord)
   â†“
7. VÃ©rifier policy (local_first)
   â”œâ”€ Si local OK â†’ retour
   â”œâ”€ Si local Ã©choue + external_on_fallback=true â†’ fallback OpenAI
   â””â”€ Si demande externe explicite â†’ appel OpenAI
   â†“
8. Sauvegarder en mÃ©moire
   â†“
9. Retourner rÃ©ponse + trace
```

---

## ğŸ¯ CapacitÃ©s Actu elles

### âœ… ImplÃ©mentÃ©es
- [x] MÃ©moire conversationnelle (5 tours)
- [x] Analyse d'intentions (urgence, keywords, entitÃ©s)
- [x] RAG TF-IDF avec 3 documents indexÃ©s
- [x] GÃ©nÃ©ration locale (fallback templates)
- [x] Provider OpenAI (gpt-4o-mini)
- [x] Policy local_first + fallback automatique
- [x] Logging dÃ©taillÃ© (JSONL quotidien)
- [x] Health check + Ã©vÃ©nements
- [x] UI Desktop prioritaire

### ğŸš§ Extensions Futures
- [ ] Indexer plus de documents (corpus extensible)
- [ ] IntÃ©gration complÃ¨te PySide6 (chat + moniteur)
- [ ] Dashboard web temps rÃ©el
- [ ] Support LM Studio local (OpenAI-compatible)
- [ ] Agents spÃ©cialisÃ©s (ego, sensorium, superego)
- [ ] Vector database (remplacer TF-IDF)

---

## ğŸ› Troubleshooting

### L'API ne dÃ©marre pas
```powershell
# VÃ©rifier l'import
C:\ElyonEU\api\.venv\Scripts\python.exe -c "from api import elyon_api; print('OK')"

# VÃ©rifier la syntax
python -m py_compile C:\ElyonEU\api\elyon_api.py
```

### Erreur 500 sur `/chat`
VÃ©rifier les logs dans la fenÃªtre API (les `[api] [DEBUG n]` affichent l'Ã©tape en erreur).

### OpenAI retourne 400
VÃ©rifier :
- ClÃ© API valide : `$env:OPENAI_API_KEY`
- ModÃ¨le disponible : `$env:ELYON_CHAT_MODEL`
- Messages valides (role + content requis)

### MÃ©moire vide
La mÃ©moire se remplit au fur des interactions. Premier appel â†’ pas de contexte antÃ©rieur.

---

## ğŸ“ Fichiers ClÃ©s

```
C:\ElyonEU/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ elyon_api.py           â† API principale (FastAPI)
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ memory.py          â† MÃ©moire conversationnelle
â”‚   â”‚   â”œâ”€â”€ intent.py          â† Analyse intentions
â”‚   â”‚   â”œâ”€â”€ vector_index.py    â† RAG TF-IDF
â”‚   â”‚   â””â”€â”€ llm.py             â† Interface LLM
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â””â”€â”€ generative.py      â† Routes `/gen/*`
â”‚   â””â”€â”€ requirements.txt        â† DÃ©pendances API
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ elyon_desktop.py       â† UI Desktop (PySide6)
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ generative_core.py â† GÃ©nÃ©rateur interne
â”‚   â”‚   â””â”€â”€ llm_client.py      â† Client LM Studio/OpenAI
â”‚   â””â”€â”€ requirements_app.txt    â† DÃ©pendances Desktop
â”œâ”€â”€ config/
â”‚   â””â”€â”€ chat_backend.json      â† Config chat
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ _memory/               â† Contexte conversationnel
â”‚   â”œâ”€â”€ vector_index/          â† Index TF-IDF
â”‚   â””â”€â”€ corpus/                â† Docs Ã  indexer
â”œâ”€â”€ journal/                   â† Logs JSONL quotidiens
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ chat/
â”‚       â””â”€â”€ index.html         â† UI web
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ Start-ElyonEU-All.ps1  â† DÃ©marrage complet
â”‚   â”œâ”€â”€ Start-ElyonEU-Desktop.ps1
â”‚   â””â”€â”€ Start-ElyonEU.ps1
â””â”€â”€ run_api.py                 â† Launcher API (uvicorn)
```

---

## ğŸ“ Support

**Logs en temps rÃ©el** :
```powershell
# Monitor API
Get-Content C:\ElyonEU\journal\journal_20251105.jsonl -Tail 50 -Wait
```

**VÃ©rifier l'Ã©tat du systÃ¨me** :
```powershell
Invoke-RestMethod -Uri "http://127.0.0.1:8000/self" -Method Get | ConvertTo-Json
Invoke-RestMethod -Uri "http://127.0.0.1:8000/events" -Method Get | ConvertTo-Json
```

---

**Ã‰lyonEU est prÃªt pour la production locale ! ğŸš€**
